---
title: "Progress Memo 2"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Chiemela Onuoha"
pagetitle: "PM2 Chiemela Onuoha"
date: "today"

format:
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    link-external-newwindow: true

execute:
  warning: false

from: markdown+emoji
reference-location: margin
citation-location: margin
---


```{r}
#| echo: false
#| label: load-packages-data

library(tidyverse)
library(tidymodels)
library(DT)
library(here)

# resolve conflicts
tidymodels_prefer()

```

::: {.callout-tip icon=false}

## Github Repo Link

[Chiemela's Github (chiemelaonu)](https://github.com/stat301-2-2025-winter/final-project-2-chiemelaonu.git)

:::

## Data source

[Kaggle.com](https://www.kaggle.com/datasets/ashpalsingh1525/imdb-movies-dataset/data)
Data was sourced from Kaggle.com, a site hosting thousands of datasets for the public to use. Accessed February 3rd, 2025^[The Kaggle data was compiled from IMDB.com]

## Analysis Plan
My analysis plan is to split the data with `prop = 0.8` and `strata = yeo_revenue`. I also will be using V-fold cross validation and fitting my models (Null, Baseline, OLS, Lasso, Ridge, Random Forest, and KNN) to the resamples. My V-fold cross validation consists of 5 folds and 3 repeats. I will be tuning the models that utilize hyperparameters so that I can use the most suitable hyperparamaters for my data. I currently have my null/baseline recipe, and 2 OLS recipes. I will be finishing off with at least 4 recipes.

## Recipes
The first recipe was for the null and baseline models. My first step was to impute the mean values for the `positive` and `negative` columns since there were missing values. I also imputed the mode of the `overall_sentiment` column. Next, I used `step_date()` to extract the year and month out of the date column to use as predictors. Lastly, I used `step_dummy()` to one-hot encode the `overall_sentiment` column so that all 3 types of sentiments are captured and used as predictors.  
<br>  
The second recipe I have is for my OLS model and most of the steps are the same as the null/baseline recipe, except I used `step_interact()` to create an interaction with 2 predictors I feel would work together in this predictive process, and used `step_normalize()` to scale and center the numerical predictors to prevent multicollinearity. 

### Updates on Data Since Memo 1
I initially only had 3 predictors for my models, but I have been able to conduct more manipulation of the data to allow for more usable predictors in my models. I added a column called `num_genres` that has the counts of the genres for each movie. I added columns `negative`, `positive`, and `overall_sentiment` that uses the `tidytext` package to parse the `overview` column and note which words are positive or negative. The `positive` and `negative` columns hold the counts of those kinds of words present in each observation, and the `overall_sentiment` column gives the overall sentiment: Negative, Positive, or Neutral, depending on how many of each kind was counted. Some of the observations had NA for these columns, but I imputed those values with the mean (for the `negative` and `positive` columns) and the mode (for the `overall_sentiment` column). Lastly, I added a column called `num_crew` that counts the number of crew members listed for each movie. These new columns will serve as better features for my model, in the hopes of creating more accurate and meaningful predictions.

### Fitted Results
```{r}
#| label: load-basic-fits
#| echo: false
#| results: hide

# load in training data ----
list.files(
  here("results/"),
  pattern = ".rda",
  full.names = TRUE
) |>
  map(load, envir = .GlobalEnv)
```


```{r}
#| label: fitted-results
#| echo: false


# examine results
model_results <-
  as_workflow_set(
    lm = lm_basic_fit,
    null = null_results,
    baseline = baseline_results
  ) 


basic_fits_table <- model_results |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  slice_min(mean, by = wflow_id) |>
  arrange(mean) |>
  select(
    `Model Name` = wflow_id,          
    `Metric Type` = .metric,          
    `Mean RMSE` = mean,               
    `Standard Error` = std_err,
    n              
  ) |> knitr::kable()

basic_fits_table
```

### Moving Forward
My data has been split and so far and 3 recipes have been defined. I have defined and fit the null, baseline, and one OLS model, and I have tuned and fit the Lasso model. I will be continuing with building recipes and tuning and fitting Ridge, Random Forest, and K-Nearest Neighbors. After I have tuned all of the models, I will compare the results and pick wich model produces the best RMSE. Then I will train that model to the training set and predict on the testing and assess that final model. As of right now, I do not see an issues or areas of concern, but will be stay vigilant in case any appear.


