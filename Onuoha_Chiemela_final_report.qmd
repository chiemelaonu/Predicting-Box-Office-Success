---
title: "Final Report"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Chiemela Onuoha"
pagetitle: "FR Chiemela Onuoha"
date: "today"

format:
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    link-external-newwindow: true

execute:
  warning: false

from: markdown+emoji
reference-location: margin
citation-location: margin
---


```{r}
#| echo: false
#| results: hide
#| label: load-packages-data

library(tidyverse)
library(tidymodels)
library(DT)
library(here)
library(doMC)
library(VGAM)
library(scales)


# resolve conflicts
tidymodels_prefer()


# read in original data ----
movies <- read_csv("data/imdb_movies.csv")

# read in cleandata ----
movies_data <- read_csv("data/movies_clean.csv")

# parallel processing ----
num_cores <- parallel::detectCores(logical = FALSE)
registerDoMC(cores = 6)

# load in results for workflow analysis ----
list.files(
  here("results/"),
  pattern = ".rda",
  full.names = TRUE
) |>
  map(load, envir = .GlobalEnv)

select_best(rf_tuned, metric = "rmse")

# basic workflow set
basic_model_results <-
  as_workflow_set(
    lm = lm_basic_fit,
    null = null_results,
    baseline = baseline_results,
    knn = knn_tuned_basic,
    rf = rf_tuned_basic,
    bt = bt_tuned_basic,
    en = en_tuned_basic
  ) 

# complex workflow set
model_results <-
  as_workflow_set(
    lm = lm_fit,
    null = null_results,
    baseline = baseline_results,
    knn = knn_tuned,
    rf = rf_tuned,
    bt = bt_tuned,
    en = en_tuned
  ) 

# load in data for final model analysis ----
load(here("data/movies_test.rda"))
load(here("results/final_fit.rda"))

# predictions
test_preds <- movies_test |>
  bind_cols(predict(final_fit, movies_test)) |>
  select(yeo_revenue, .pred)

test_preds_original <- test_preds |>
  mutate(
    price_actual = yeo.johnson(yeo_revenue, lambda = 0.25, derivative = 0, inverse = TRUE),   
    price_predicted = yeo.johnson(.pred, lambda = 0.25, derivative = 0, inverse = TRUE)       
  )
```

::: {.callout-tip icon=false}

## Github Repo Link

[Chiemela's Github (chiemelaonu)](https://github.com/stat301-2-2025-winter/final-project-2-chiemelaonu.git)

:::

## Introduction
The objective of this report is to present findings of predictive modeling done on a movies dataset. I would like to see how well a movies' revenue can be predicted using other features of the movie/data.
<br>  
These predictions are useful because having an accurate prediction model for movie revenue could be highly valuable for investors and filmmakers because it would help estimate a film’s financial success before release. That would allow for better budget allocation, and even optimize marketing strategies.

## Data Overview
Below is a table showing the attributes of our movies dataset, like the column type and number of missing variables. With this table, we see that there are very few missing values, and those that are missing are present in the genre and crew columns. The main purpose of this check is to see any missingness in the target variable, which is not present in this data. 
```{r}
#| label: tbl-missing
#| tbl-cap: "Skim of Missing Data"
#| echo: false

# missingness check ----
movies |>
  skimr::skim_without_charts() |>
  knitr::kable()

```

The table below provides a more concise skimming of the data, telling us that we have just under 10,200 observations, 12 columns, 126 rows with missing values.
```{r}
#| label: tbl-eda
#| tbl-cap: "Dataset Summary"
#| echo: false

# calculate values
missing_rows <- sum(!complete.cases(movies))
num_rows <- nrow(movies)
num_cols <- ncol(movies)
target_missing <- sum(!complete.cases(movies$revenue))

# create a table
summary_table <- tibble(
  Metric = c("Rows with Missing Values", "Number of Observations", "Number of Variables", "Number of Missing Target Variable Observations (`revenue`)"),
  Value = c(missing_rows, num_rows, num_cols, target_missing)
) |> knitr::kable()


summary_table
```

::: {#fig-rev-dists layout-ncol=2}

![revenue on Original Scale ($)](figures/target_eda.png){#fig-revenue-orig}

![revenue Transformed with Yeo-Johnson](figures/target_eda_2.png){#fig-revenue-yj}

:::
On the left, we have the original distribution of the target variable, revenue, visualized with a density plot and box plot. 
The original values were heavily skewed right, so it was apparent a transformation of revenue was needed. After experimenting with log, Yeo-Johnson, square root, and Box-Cox transformations, I settled on a Yeo-Johnson transformation to make the values more evenly distributed.^[A lambda value of 0.25 was used]  
<br>  
The Yeo-Johnson transformation reshapes data to make it more normal by adjusting values differently based on whether they are positive or negative. For positive numbers, it applies a power transformation or a log function, while for negative numbers, it flips them, transforms them, and flips them back. The density and box plot on the right shows those values post-transformation. 

### Data Wrangling
The movies dataset initially had 12 columns and I was only able to extract 3 predictors for my models, but I have been able to conduct more manipulation of the data to allow for more usable predictors in my models. 
```{r}
#| label: tbl-modified-data
#| tbl-cap: "Movies Data with Added Columns"
#| echo: false

movies_data |>
  glimpse()
```

In the table above, you can see that I added a column called `num_genres` that has the counts of the genres for each movie. I added columns `negative`, `positive`, and `overall_sentiment` that uses the `tidytext` package to parse the `overview` column and note which words are positive or negative. The `positive` and `negative` columns hold the counts of those kinds of words present in each observation, and the `overall_sentiment` column gives the overall sentiment: Negative, Positive, or Neutral, depending on how many of each kind was counted. Lastly, I added a column called `num_crew` that counts the number of crew members listed for each movie. These new columns will serve as better features for my model, in the hopes of creating more accurate and meaningful predictions.

## Methods

### Data Splitting
I decided on a split proportion of 80/20 because it gives an optimal amount of data in the testing and training sets. I also used the default number of strata and stratified by our target variable, yeo_revenue (the Yeo-Johnson transformed revenue).

#### Resampling
I will be using V-fold cross-validation for resampling, with 5 folds (V) and 3 repeats. This method is useful because it ensures that each data point is used for both training and testing, providing a more reliable estimate of model performance.

### Metrics
The metric I used for comparing and selecting a final model is Mean Absolute Error (MAE). MAE measures the average magnitude of the errors between the predicted and actual values, without considering their direction. I picked MAE as my metric because it is commonly used in regression modeling and is a relatively simple metric to interpret and explain.


### Model Descriptions
This is a regression problem because the target variable, revenue, is continuous rather than categorical. Since it is a regression problem, I knew I would like to fit/tune a Null/Baseline model, an OLS model, Elastic Net models, a Random Forest Model, a Boosted Trees model, and a K-Nearest Neighbors model.

#### Null/Baseline
A null model is a simple model that based solely on the mean (for regression). It serves as a basic reference to compare the performance of more complex models.  
<br>  
A baseline model is a simple predictive model used as a benchmark. Its use is to set a reasonable lower bound for predictive performance. Comparing against a baseline helps measure whether advanced modeling techniques provide meaningful improvement.


#### OLS
An Ordinary Least Squared (OLS) model is used for when relationships are linear, features are independent, and interpretability is needed. An OLS model captures linear relationships, effect sizes, and predictor significance well, and is a almost always one of the first models to be used in a regression problem. It provides another fairly simple benchmark for our model.


#### Elastic Net
Elastic Net is a regularized regression method that combines Ridge and Lasoo penalties. It stabilizes coefficient estimates like Ridge while also eliminating irrelevant variables like Lasso. Elastic Net is particularly useful when predictors are highly correlated, when there are more features than observations, or when handling noisy or sparse data.  
<br>  
In Elastic Net, the `penalty` controls how much the model shrinks the coefficients, with higher values forcing them closer to zero. The `mixture` decides the balance between Lasso, which can set some coefficients to zero, and Ridge, which shrinks them without making any exactly zero. A mixture of 1 is pure Lasso, 0 is pure Ridge, and values in between blend both.  
<br>  
The hyperparameters I will be tuning are `mixture` and `penalty`.

#### Random Forest
Random Forest is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and reduce overfitting. It is ideal for capturing nonlinear relationships and handling high-dimensional data.  
<br>  
In Random Forest, `mtry` is the number of features randomly chosen at each split, affecting how diverse the trees are. `min_n` is the minimum number of data points needed in a node before it can split, controlling how deep the trees grow.  
<br>  
The hyperparameters I will be tuning are `min_n` with a range of 2-40, and `mtry` with a range of 1-10.

#### Boosted Trees
Boosted trees is a method that builds decision trees sequentially, where each tree corrects the errors of the previous one, leading to high predictive accuracy and strong performance on complex datasets.  
<br>  
In Boosted Trees, we have `mtry` and `min_n` as hyperparamaters as well. `learn_rate` controls how much each tree contributes to the final prediction, with lower values leading to slower but more stable learning. `tree_depth` sets how deep each tree can grow, balancing complexity and overfitting.  
<br>  
The hyperparameters I will be tuning are `min_n` with a range of 2-40, `mtry` with a range of 1-10, `learn_rate` with a range of -5 to -0.2, and a `tree_depth` range of 6 to 10.

#### K-Nearest Neighbors
K-Nearest Neighbors (KNN) is a non-parametric model that classifies or predicts values based on the majority vote of nearby data points or their average. It works well for nonlinear relationships.  
<br>  
The hyperparameter I will be tuning is `neighbors`.  
<br>  
In K-Nearest Neighbors (KNN), the `neighbors` parameter sets how many nearby points the model looks at when making a prediction. A small values makes the model sensitive to noise, while a large value smooths predictions but may miss local patterns. Choosing the right value balances accuracy and generalization.

::: {.callout-important}
There are no hyperparamaters to tune for the Null/Baseline or OLS models, as the model is fit to the resamples as normal.
:::


### Recipes
There are 2 recipes defined for each model (or set of models in the case of the Elastic Net models), excluding the Null/Baseline models that use only one recipe. There is each a "basic" recipe and a "complex" recipe used for the models. 

#### Null/Baseline Model Recipe
The first recipe was for the null and baseline models. My first step was to impute the mean of all missing numerical values. I also imputed the mode of the `overall_sentiment` column because of NAs. Next, I used `step_date()` to extract the year and month out of the date column to use as predictors. Lastly, I used `step_dummy()` to one-hot encode the `overall_sentiment` column so that all 3 types of sentiments are captured and used as predictors.  

#### Linear Model Recipes
For my complex Linear model recipe, most of the steps are the same as the null/baseline recipe, except I used `step_interact()` to create an interaction with 2 sets of predictors I feel would work together in this predictive process, and used `step_normalize()` to scale and center the numerical predictors to prevent multicollinearity. I also added a Yeo-Johnson transformation of the `budget_x` column. For my basic Linear model recipe has everything previously stated, except the interactions and the `budget_x` transformation.  

#### Tree Based Model Recipes  
For my basic tree model recipe, I kept the same steps as my basic Linear model recipe. For my complex recipe, I created a new column called `season` that splits up the months into the 4 seasons and used that as a predictor as well.

## Model Building and Selection Results

### Basic Workflows
```{r}
#| label: tbl-rf-basic
#| tbl-cap: "Basic Random Forest Best Hyperparameters"
#| echo: false


select_best(rf_tuned_basic, metric = "mae") |>
  mutate(Model_Type = "Random Forest") |>  
  select(-.config) |>
  knitr::kable()


```
::: {#fig-rf layout-ncol=1}

![Basic Random Forest Workflow Plot ](figures/rf_basic_auto.png){#fig-rf-basic}
:::

The Random Forest Workflow's best hyperparameters are an `mtry` of 10 and `min_n` of 11. The MAE had a steep decline in the beginning and began to level out at an `mtry` of 5.

```{r}
#| label: tbl-knn-basic
#| tbl-cap: "Basic KNN Best Hyperparameters"
#| echo: false


select_best(knn_tuned_basic, metric = "mae") |>
  mutate(Model_Type = "K-Nearest Neighbor") |>  
  select(-.config) |>
  knitr::kable()

```
::: {#fig-knn layout-ncol=1}
![Basic K-Nearest Neighbor Workflow Plot ](figures/knn_basic_auto.png){#fig-knn-basic}
:::
@fig-knn and @tbl-knn-basic show that the KNN best hyperparamters are a `neighbors` value of 10. 



```{r}
#| label: tbl-bt-basic
#| tbl-cap: "Basic Boosted Trees Best Hyperparameters"
#| echo: false


select_best(bt_tuned_basic, metric = "mae") |>
  mutate(Model_Type = "Boosted Trees") |>  
  select(-.config) |>
  knitr::kable()

```


```{r}
#| label: tbl-en-basic
#| tbl-cap: "Basic Elastic Net Best Hyperparameters"
#| echo: false

select_best(en_tuned_basic, metric = "mae") |>
  mutate(Model_Type = "Elastic Net") |>  
  select(-.config) |>
  knitr::kable()
```
::: {#fig-en layout-ncol=1}
![Basic Elastic Net Workflow Plot ](figures/en_basic_auto.png){#fig-en-basic}
:::
Elastic Net's best hyperparameters are a `mixture` of 1 and a `penalty` of 0.5, which is a Lasso model. 

::: {#fig-result layout-ncol=1}

![Basic Workflow Plot ](figures/basic_autoplot.png){#fig-plot-basic}
:::
@fig-result shows the distribution of the RMSE's of the "basic" model workflows, along with the null and baseline models. The sharp decline from the null model to the rest of the models is evidence that more complex models and recipes are needed to produce better predictions. Additionally, among the complex models, the RMSE is still decreasing, though the change is not much between a few models. Since the changes between the Baseline, KNN, OLS, and EN model are not very significant, we learn that we should try to be pickier about the models we want to tune/fit, since the computational toll is not worth it for those models. On the other hand, Random Forest and Boosted Trees have a significant decrease from the 4 aforementioned workflows, and that proves that tuning those models were helpful and worth the longer times it took to run them. 
```{r}
#| label: tbl-basic-fits
#| tbl-cap: "Basic Model Results"
#| echo: false
basic_model_results |>
  collect_metrics() |>
  filter(.metric == "mae") |>
  slice_min(mean, by = wflow_id) |>
  arrange(mean) |>
  select(
    `Model Type` = wflow_id,
    MAE = mean,
    `Std Error` = std_err, n = n
  ) |>
  knitr::kable(digits = 4)
```
@tbl-basic-fits shows the more precise MAE's for the "basic" workflows. The MAE of null model is significantly higher than the rest, at about 149. KNN has an MAE of about 93.5. The baseline, EN, and Linear model are all around the same MAE values of about 93.8. Then, Random Forest and Boosted Trees have a significant drop to around 73.9 and 73, respectively.

### Complex Workflows

```{r}
#| label: tbl-rf-complex
#| tbl-cap: "Complex Random Forest Best Hyperparameters"
#| echo: false

select_best(rf_tuned, metric = "mae") |>
  mutate(Model_Type = "Random Forest") |>  
  select(-.config) |>
  knitr::kable()
```
::: {#fig-rf-complex layout-ncol=1}
![Complex Random Forest Workflow Plot ](figures/rf_auto.png){#fig-rf-complex}
:::
The complex Random Forest workflow's best hyperparamters are an `mtry` of 10 and an `min_n` of 11.

```{r}
#| label: tbl-knn-complex
#| tbl-cap: "Complex KNN Best Hyperparameters"
#| echo: false

select_best(knn_tuned, metric = "mae") |>
  mutate(Model_Type = "K-Nearest Neighbor") |>  
  select(-.config) |>
  knitr::kable()
```
::: {#fig-knn-complex layout-ncol=1}
![K-Nearest Neighbor Complex Workflow Plot ](figures/knn_auto.png){#fig-knn-complex}
:::
The complex KNN workflow's best hyperparameters is a `neighbors` value of 10.

```{r}
#| label: tbl-bt-complex
#| tbl-cap: "Complex Boosted Tree Best Hyperparameters"
#| echo: false

select_best(bt_tuned, metric = "mae") |>
  mutate(Model_Type = "Boosted Trees") |>  
  select(-.config) |>
  knitr::kable()
```





```{r}
#| label: tbl-en-complex
#| tbl-cap: "Complex Elastic Net Best Hyperparameters"
#| echo: false

select_best(en_tuned, metric = "mae") |>
  mutate(Model_Type = "Elastic Net") |>  
  select(-.config) |>
   knitr::kable()
```
::: {#fig-en-complex layout-ncol=1}
![Elastic Net Complex Workflow Plot ](figures/en_auto.png){#fig-en-complex}
:::
The complex Elastic Net workflow's best hyperparameters are a `penalty` of 0.75 and an `mixture` of 1, which is a Lasso model again.

::: {#fig-result-complex layout-ncol=1}
![Complex Workflow Plot ](figures/complex_autoplot.png){#fig-plot-complex}
:::

@fig-result-complex shows the distribution of the MAE's of the "complex" model workflows, along with the null and baseline models. The plot paints a very similar picture compared to the "basic" workflows, but the most notable difference is that the Elastic Net and OLS models perform significantly worse than their basic workflows. That could mean the feature engineering I conducted in the complex linear recipes were not a very good fit for those models.

```{r}
#| label: tbl-complex-results
#| tbl-cap: "Complex Model Results"
#| echo: false
model_results |>
  collect_metrics() |>
  filter(.metric == "mae") |>
  slice_min(mean, by = wflow_id) |>
  arrange(mean) |>
  select(
    `Model Type` = wflow_id,
    MAE = mean,
    `Std Error` = std_err, n = n
  ) |>
  knitr::kable(digits = 4)
```
Boosted Trees and Random Forest have the lowest MAE's and they are within one standard deviation of each other, so we can pick either one of those models for the final analysis. Since the standard errors for the Boosted Trees model are lower, I will be picking Boosted Trees as the best model. Additionally, since the basic Boosted Tree model workflow has a lower standard error, I will specifically be using that workflow for the final model analysis.

### Differences in Performance Between Model Types and Recipes
The EN and OLS models were fit with the same set of recipes, a more "basic" one and a more "complex" one. Their performance is around the same, which should make sense since they are both linear regression and using sets of recipes. What interests me is that the Baseline model performs very similarly to them in the basic workflow, even though it was fit with a different recipe. Granted, for the basic EN and OLS workflows, the recipes used were not actually different from the null/baseline recipe used to fit the baseline model. But for the complex EN and OLS recipes, I added interactions and a transformation of the budget column, which seemingly made them perform worse in general, and worse than the baseline model.  
<br>  
Another interesting point is the KNN model. Though I used the same set of the recipes to fit the KNN, Boosted Tree and Random Forest Workflows. KNN performed significantly worse than the other 2. That could mean that I did not perform adequate tuning, the recipe needs changes, or the predictors and/or the data as a whole does not work well with the model. In the future, it could be meaningful to change the way I tuned the KNN model or possibly find different ways to change the recipe so that the model could perform better, though those changes are not a guarantee the KNN model would perform better.  
<br>  
In the end, I am not surprised Boosted Trees and Random Forest did the best since their capabilities in predictive modeling are more robust and overall capture many different aspects of the data when producing predictions. 

## Final Model Analysis
After training the final model (Boosted Trees) and predicting on the test set, I calculated MAE, as well as R-squared and RMSE to provide more context on the model's predictions. Additionally, since we used `yeo_revenue` as the target variable and not the original `revenue`, I will be transforming the values back to the original scale so that meaningful deductions can be made with the MAE.

```{r}
#| label: tbl-final-metrics
#| tbl-cap: "Final Assessment Metrics (on Yeo-Johnson Scale)"
#| echo: false

# compute metrics
rmse_result <- rmse(test_preds, truth = yeo_revenue, estimate = .pred)
mae_result <- mae(test_preds, truth = yeo_revenue, estimate = .pred)
rsq_result <- rsq(test_preds, truth = yeo_revenue, estimate = .pred)

# create a table
metrics_table <- tibble(
  Metric = c("RMSE", "MAE", "R²"),
  Value = c(rmse_result$.estimate, mae_result$.estimate, rsq_result$.estimate)
) |> knitr::kable()

# print table
metrics_table
```
@tbl-final-metrics says that the RMSE is at a 99.2, while the MAE is at 73.4. When calculating RMSE and MAE, we are looking for lower values that show our predicted values are not too far off from the actual values. The model produces a better MAE value than RMSE which could be because of the RMSE squaring already large values, making them much larger and distorting the scale.

```{r}
#| label: tbl-final-metrics-orig
#| tbl-cap: "Final Assessment Metrics (on Original Scale)"
#| echo: false

# compute metrics
rmse_orig <- rmse(test_preds_original, truth = price_actual, estimate = price_predicted) 
mae_orig <- mae(test_preds_original, truth = price_actual, estimate = price_predicted) 
rsq_orig <- rsq(test_preds_original, truth = price_actual, estimate = price_predicted) 


# create a table
metrics_table_orig <- tibble(
  Metric = c("RMSE", "MAE", "R²"),
  Value = c(rmse_orig$.estimate, mae_orig$.estimate, rsq_orig$.estimate)
) |>
  mutate(Value = format(Value, scientific = FALSE, big.mark = ",")) |>
  knitr::kable()

metrics_table_orig 

```
On the original scale in @tbl-final-metrics-orig, the numbers are very high (though that can be expected since movie revenue can go up to the hundreds of millions). The original scale allows us to better interpret the model predictions, showing that the RMSE of 99.2 is equivalent to almost \$170 million and the MAE of 73.4 is equivalent to \$100.4 million. The Rsquared also reduces slightly because of the scaling changes, which is not a very good sign as it was already at 66.7%. The 66.7% means that the model only explains 66.7% of the variability in the results, which is not a very good number. We would prefer the number to be higher, as that means the model is capturing the variance well and adjusting accordingly.

::: {#fig-final-plot-yeo layout-ncol=1}
![Final Predictions Plot (Yeo-Johnson)](figures/final_plot_yeo.png){#fig-plot}
:::
@fig-final-plot-yeo shows that the predicted revenue with the actual revenue on the Yeo-Johnson scale. The Boosted Trees model was able to make some correct or almost correct predictions, but there are still many predictions that are far off the actual values. 

::: {#fig-final-plot-orig layout-ncol=1}
![Final Predictions Plot (Original)](figures/final_plot_orig.png){#fig-plot-orig}
:::
@fig-final-plot-orig has the predicted revenue with the actual revenue on the original scale. 

::: {#fig-final-plot-orig-zoom layout-ncol=1}
![Final Predictions Plot (Original)](figures/final_plot_orig_zoom.png){#fig-plot-zoom}
:::
To better see the outcomes of the model, @fig-final-plot-orig-zoom is the distribution of the previous @fig-final-plot-orig but restricted to between 0 and 500,000,000. This plot shows that there is a bit more overfitting then underfitting present (at least for this region) which can make sense since it is a Boosted Tree model and they can be prone to overfitting.

::: {#fig-final-plot-yeo-zoom layout-ncol=1}
![Final Predictions Plot (Yeo-Johnson)](figures/final_plot_yeo_zoom.png){#fig-plot-zoom-yeo}
:::
@fig-final-plot-yeo-zoom is the distribution of @fig-final-plot-yeo but between the range of 250 and 500, one of the clustered regions on the full dimension plot. The less-than-ideal performance of this model was foreshadowed by the R-squared value, which could mean that this prediction problem is difficult to capture in modeling. There could have been inadequate feature engineering as well, which would need to be another area of focus if exploring the reasons for the poor predictions.

## Conclusion
