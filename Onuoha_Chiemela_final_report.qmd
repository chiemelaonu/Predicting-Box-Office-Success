---
title: "Final Report"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Chiemela Onuoha"
pagetitle: "FR Chiemela Onuoha"
date: "today"

format:
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    link-external-newwindow: true

execute:
  warning: false

from: markdown+emoji
reference-location: margin
citation-location: margin
---


```{r}
#| echo: false
#| results: hide
#| label: load-packages-data

library(tidyverse)
library(tidymodels)
library(DT)
library(here)
library(doMC)

# resolve conflicts
tidymodels_prefer()


# load in data ----
movies <- read_csv("data/imdb_movies.csv")
# read in data ----
movies_data <- read_csv("data/movies_clean.csv")

# parallel processing ----
num_cores <- parallel::detectCores(logical = FALSE)
registerDoMC(cores = 6)

# load in results ----
list.files(
  here("results/"),
  pattern = ".rda",
  full.names = TRUE
) |>
  map(load, envir = .GlobalEnv)

select_best(rf_tuned, metric = "rmse")

# basic workflow set
basic_model_results <-
  as_workflow_set(
    lm = lm_basic_fit,
    null = null_results,
    baseline = baseline_results,
    knn = knn_tuned_basic,
    rf = rf_tuned_basic,
    bt = bt_tuned_basic,
    en = en_tuned_basic
  ) 

# complex workflow set
model_results <-
  as_workflow_set(
    lm = lm_fit,
    null = null_results,
    baseline = baseline_results,
    knn = knn_tuned,
    rf = rf_tuned,
    bt = bt_tuned,
    en = en_tuned
  ) 
```

::: {.callout-tip icon=false}

## Github Repo Link

[Chiemela's Github (chiemelaonu)](https://github.com/stat301-2-2025-winter/final-project-2-chiemelaonu.git)

:::

## Introduction
The objective of this report is to present findings of predictive modeling done on a movies dataset. I would like to see how well a movies' revenue can be predicted using other features of the movie/data.
<br>  
These predictions are useful because having an accurate prediction model for movie revenue could be highly valuable for investors and filmmakers because it would help estimate a filmâ€™s financial success before release. That would allow for better budget allocation, and even optimize marketing strategies.

## Data Overview
Below is a table showing the attributes of our movies dataset, like the column type and number of missing variables. With this table, we see that there are very few missing values, and those that are missing are present in the genre and crew columns. The main purpose of this check is to see any missingness in the target variable, which is not present in this data. 
```{r}
#| label: tbl-missing
#| tbl-cap: "Skim of Missing Data"
#| echo: false

# missingness check ----
movies |>
  skimr::skim_without_charts() |>
  knitr::kable()

```

The table below provides a more concise skimming of the data, telling us that we have just under 10,200 observations, 12 columns, 126 rows with missing values.
```{r}
#| label: tbl-eda
#| tbl-cap: "Dataset Summary"
#| echo: false

# calculate values
missing_rows <- sum(!complete.cases(movies))
num_rows <- nrow(movies)
num_cols <- ncol(movies)
target_missing <- sum(!complete.cases(movies$revenue))

# create a table
summary_table <- tibble(
  Metric = c("Rows with Missing Values", "Number of Observations", "Number of Variables", "Number of Missing Target Variable Observations (`revenue`)"),
  Value = c(missing_rows, num_rows, num_cols, target_missing)
) |> knitr::kable()


summary_table
```

::: {#fig-rev-dists layout-ncol=2}

![revenue on Original Scale ($)](figures/target_eda.png){#fig-revenue-orig}

![revenue Transformed with Yeo-Johnson](figures/target_eda_2.png){#fig-revenue-yj}

:::
On the left, we have the original distribution of the target variable, revenue, visualized with a density plot and box plot. The original values were heavily skewed right, so it was apparent a transformation of revenue was needed. After experimenting with log, Yeo-Johnson, square root, and Box-Cox transformations, I settled on a Yeo-Johnson transformation to make the values more evenly distributed.^[A lambda value of 0.25 was used] The density and box plot on the right shows those values post-transformation. 

### Data Wrangling
The movies dataset initially had 12 columns and I was only able to extract 3 predictors for my models, but I have been able to conduct more manipulation of the data to allow for more usable predictors in my models. 
```{r}
#| label: tbl-modified-data
#| tbl-cap: "Movies Data with Added Columns"
#| echo: false

movies_data |>
  glimpse()
```

In the table above, you can see that I added a column called `num_genres` that has the counts of the genres for each movie. I added columns `negative`, `positive`, and `overall_sentiment` that uses the `tidytext` package to parse the `overview` column and note which words are positive or negative. The `positive` and `negative` columns hold the counts of those kinds of words present in each observation, and the `overall_sentiment` column gives the overall sentiment: Negative, Positive, or Neutral, depending on how many of each kind was counted. Some of the observations had NA for these columns, but I imputed those values with the mean (for the `negative` and `positive` columns) and the mode (for the `overall_sentiment` column). Lastly, I added a column called `num_crew` that counts the number of crew members listed for each movie. These new columns will serve as better features for my model, in the hopes of creating more accurate and meaningful predictions.

## Methods

### Data Splitting
I decided on a split proportion of 80-20 because it gives an optimal amount of data in the testing and training sets. I also used the default number of strata and stratified by our target variable, yeo_revenue (the Yeo-Johnson transformed revenue).

#### Resampling
I will be using V-fold cross-validation for resampling, with 5 folds (V) and 3 repeats. This method is useful because it ensures that each data point is used for both training and testing, providing a more reliable estimate of model performance.

### Metrics
The metric I used for comparing and selecting a final model is RMSE. RMSE is the Root Mean Squared Error which measures the square root of the average squared differences between the predicted and actual values, providing a way to quantify how much the model's predictions deviate from the true values. I picked RMSE as my metric because it is commonly used in regression modeling and is a relatively simple metric to interpret and explain.



### Model Descriptions
This is a regression problem because the target variable, revenue, is continuous rather than categorical. Since it is a regression problem, I knew I would like to fit/tune a Null/Baseline model, an OLS model, Elastic Net models, a Random Forest Model, a Boosted Trees model, and a K-Nearest Neighbors model.

#### Null/Baseline
A null model is a simple model that based solely on the mean (for regression). It serves as a basic reference to compare the performance of more complex models.  
<br>  
A baseline model is a simple predictive model used as a benchmark. Its use is to set a reasonable lower bound for predictive performance. Comparing against a baseline helps measure whether advanced modeling techniques provide meaningful improvement.


#### OLS
An Ordinary Least Squared (OLS) model is used for when relationships are linear, features are independent, and interpretability is needed. An OLS model captures linear relationships, effect sizes, and predictor significance well, and is a almost always one of the first models to be used in a regression problem. It provides another fairly simple benchmark for our model.

#### Elastic Net
Elastic Net is a regularized regression method that combines Ridge and Lasoo penalties. It stabilizes coefficient estimates like Ridge while also eliminating irrelevant variables like Lasso. Elastic Net is particularly useful when predictors are highly correlated, when there are more features than observations, or when handling noisy or sparse data.  
<br>  
The parameters I will be tuning are mixture and penalty.

#### Random Forest
Random Forest is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and reduce overfitting. It is ideal for capturing nonlinear relationships and handling high-dimensional data.  
<br> 
The parameters I will be tuning are mtry, and min_n.

#### Boosted Trees
Boosted trees is a method that builds decision trees sequentially, where each tree corrects the errors of the previous one, leading to high predictive accuracy and strong performance on complex datasets. They do well at capturing nonlinear relationships, handling feature importance, and reducing bias, making them ideal for structured data with intricate patterns.  
<br>  
The parameters I will be tuning are min_n, mtry, and learn_rate.

#### K-Nearest Neighbors
K-Nearest Neighbors is a non-parametric model that classifies or predicts values based on the majority vote of nearby data points or their average. It works well for nonlinear relationships.  
<br>  
The parameters I will be tuning are neighbors.

### Recipes
There are 2 recipes defined for each model (or set of models in the case of the Elastic Net models), excluding the Null/Baseline models that use only one recipe. There is each a "basic" recipe and a "complex" recipe used for the models. 

#### Null/Baseline Model Recipe
The first recipe was for the null and baseline models. My first step was to impute the mean of all missing numerical values. I also imputed the mode of the `overall_sentiment` column because of NAs. Next, I used `step_date()` to extract the year and month out of the date column to use as predictors. Lastly, I used `step_dummy()` to one-hot encode the `overall_sentiment` column so that all 3 types of sentiments are captured and used as predictors.  

#### Linear Model Recipes
For my complex Linear model recipe, most of the steps are the same as the null/baseline recipe, except I used `step_interact()` to create an interaction with 2 sets of predictors I feel would work together in this predictive process, and used `step_normalize()` to scale and center the numerical predictors to prevent multicollinearity. I also added a Yeo-Johnson transformation of the `budget_x` column. For my basic Linear model recipe has everything previously stated, except the interactions and the `budget_x` transformation.  

#### Tree Based Model Recipes  
For my basic tree model recipe, I kept the same steps as my basic Linear model recipe. For my complex recipe, I created a new column called `season` that splits up the months into the 4 seasons and used that as a predictor as well.

## Model Building and Selection Results
::: {#fig-result layout-ncol=1}

![Basic Workflow Plot ](figures/basic_autoplot.png){#fig-plot}
:::
@fig-result shows the distribution of the RMSE's of the "basic" model workflows, along with the null and baseline models. The sharp decline from the null model to the rest of the models is evidence that more complex models and recipes are needed to produce better predictions. Additionally, among the complex models, the RMSE is still decreasing, though the change is not much between a few models. Since the changes between the Baseline, KNN, OLS, and EN model are not very significant, we learn that we should try to be pickier about the models we want to tune/fit, since the computational toll is not worth it for those models. On the other hand, Random Forest and Boosted Trees have a significant decrease from the 4 aforementioned workflows, and that proves that tuning those models were helpful and worth the longer times it took to run them. 
```{r}
#| label: tbl-basic-fits
#| tbl-cap: "Basic Fits Table"
#| echo: false
basic_model_results |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  slice_min(mean, by = wflow_id) |>
  arrange(mean) |>
  select(
    `Model Type` = wflow_id,
    Accuracy = mean,
    `Std Error` = std_err, n = n
  ) |>
  knitr::kable(digits = 4)
```
@tbl-basic-fits shows the more precise RMSE's for the "basic" workflows. The RMSE of null model is significantly higher than the rest, at about 175. KNN has an RMSE of about 122. The baseline, EN, and Linear model are all around the same RMSE values in the 120 range. Then, Random Forest and Boosted Trees have a significant drop to around 100. The gap between Random Forest and Boosted Trees is quite miniscule, at less than one standard deviation apart.


::: {#fig-result-complex layout-ncol=1}
![Complex Workflow Plot ](figures/complex_autoplot.png){#fig-plot}
:::

@fig-result-complex shows the distribution of the RMSE's of the "complex" model workflows, along with the null and baseline models. The plot paints a very similar picture compared to the "basic" workflows, but the most notable difference is that the Baseline model performs significantly worse than the EN and OLS models, though it still fairly close to the KNN model. Between the KNN and Baseline model, there is a standard deviation of more than 1, so the difference, though small in a general sense, is significant. The rest of the models perfom about the same.

```{r}
#| label: tbl-complex-results
#| tbl-cap: "Complex Model Results"
#| echo: false
model_results |>
  collect_metrics() |>
  filter(.metric == "rmse") |>
  slice_min(mean, by = wflow_id) |>
  arrange(mean) |>
  select(
    `Model Type` = wflow_id,
    Accuracy = mean,
    `Std Error` = std_err, n = n
  ) |>
  knitr::kable(digits = 4)
```
With the "complex" models performing about the same as the "basic" models, we can choose either Random Forest or Boosted Trees as the best model. The difference between the is less than 1 standard deviation in both cases, so either is usable for a final model analysis. Since the standard errors for the Boosted Trees model are lower, I will be picking Boosted Trees as the best model. Additionally, since the basic Boosted Tree model workflow has a lower standard error, I will specifically be using that workflow for the final model analysis.

### Differences in Performance Between Model Types and Recipes
The EN and OLS models were fit with the same set of recipes, a more "basic" one and a more "complex" one. Their performance is around the same, which should make sense since they are both linear regression and using sets of recipes. What interests me is that the Baseline model performs very similarly to them, even though it was fit with a different recipe. Granted, for the basic EN and OLS workflows, the recipes used were not actually different from the null/baseline recipe used to fit the baseline model. But for the complex EN and OLS recipes, I added interactions and a transformation of the budget column, which seemingly made them perform worse.  
<br>  
Another interesting point is the KNN model. Though I used the same set of the recipes to fit the KNN, Boosted Tree and Random Forest Workflows. KNN performed significantly worse than the other 2. That could mean that I did not perform inadequate tuning, the recipe needs changes, or the predictors and/or the data as a whole do not work well with the model. In the future, it could be meaningful to change the way I tuned the KNN model or possibly find different ways to change the recipe so that the model could perform better, though those changes are not a guarantee the KNN model would perform better.  
<br>  
In the end, I am not surprised Boosted Trees and Random Forest did the best since their capabilities in predictive modeling are more robust. They handle multi-dimensional data very well, they handle non-linear data without needing to do too much feature engineering, and overall captures many different aspects of the data when producing predictions. 

## Final Model Analysis
After training the final model (Boosted Trees) and predicting on the test set, the RMSE value we end up with is 99.2. Since we used `yeo_revenue` as the target variable and not the original `revenue`, it would be useful to transform the values back to the original scale so that meaningful deductions can be made with the RMSE.  
::: {#fig-final-plot layout-ncol=1}
![Final Predictions Plot](figures/final_plot.png){#fig-plot}
:::