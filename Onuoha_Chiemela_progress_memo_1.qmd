---
title: "Progress Memo 1"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Chiemela Onuoha"
pagetitle: "PM1 Chiemela Onuoha"
date: "January 25, 2025"

format:
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    link-external-newwindow: true

execute:
  warning: false

from: markdown+emoji
reference-location: margin
citation-location: margin
---


```{r}
#| echo: false
#| label: load-packages-data

library(tidyverse)

cancer_data <- read_csv("data/pancreatic_cancer_prediction_sample.csv")

```

::: {.callout-tip icon=false}

## Github Repo Link

[Chiemela's Github (chiemelaonu)](https://github.com/stat301-2-2025-winter/final-project-2-chiemelaonu.git)

:::

## Data source

[Kaggle.com](https://www.kaggle.com/datasets/raedaddala/top-500-600-movies-of-each-year-from-1960-to-2024/code)
Data was sourced from Kaggle.com, a site hosting thousands of datasets for the public to use. Accessed January 31st 2025


## Prediction Problem
The goal of this study is to predict pancreatic cancer survival based on demographic, lifestyle, and medical history variables. This is a binary regressions problem, where the target variable is `Survival_Time_Months`. When building my predictive model, I will be using observations where the person survived longer than 18 months.
<br>  
Pancreatic cancer has one of the lowest survival rates among cancers, so being able to predict survival outcomes could be really useful. A good prediction model could help doctors identify high-risk patients earlier and plan treatments accordingly. It could also provide insight into what factors influence survival the most.  


## Data Quality Check


The data has 24 variables, with 50,000 observations. It has 9 categorical variables, and 15 numerical variables.


```{r}
#| label: tbl-summary
#| tbl-cap: "Dataset Summary"
#| echo: false


# Calculate values
missing_rows <- sum(!complete.cases(cancer_data))
num_rows <- nrow(cancer_data)
num_cols <- ncol(cancer_data)

# Create a table
summary <- data.frame(
  Metric = c("Rows with Missing Values", "Number of Observations", "Number of Variables"),
  Value = c(missing_rows, num_rows, num_cols)
)

# Display as a table
summary_table <- knitr::kable(summary, caption = "Dataset Summary")
summary_table

```
There is no missingness in the data and the data does appear to be tidy.

### Variable Types 

```{r}
#| label: variable-types
#| echo: false

# Count variable types
variable_types <- sapply(cancer_data, class)

# Summarize counts
num_numerical <- sum(variable_types %in% c("integer", "numeric"))
num_categorical <- sum(variable_types %in% c("factor", "character"))

# Print results
cat("Numerical Variables:", num_numerical, "\n")
cat("Categorical Variables:", num_categorical, "\n")
```



## Target Variable Analysis

::: {#fig-price-dists layout-ncol=2}

![Survival_Time_Months on Original Scale ($)](figures/graphic_1.png){#fig-profit-orig}

![Survival_Time_Months Transformed to log 10 Scale](figures/graphic_2.png){#fig-profit-log10}

Inspecting target distribution `Survival_Time_Months` on original scale and on a log 10 transformation scale.

:::

```{r}
#| label: missingness-check
#| tbl-cap: "Target Variable Check"
#| echo: false

cancer_data |>
  select(Survival_Time_Months) |>
  skimr::skim_without_charts() |>
  knitr::kable()
```
The target variable is originally skewed right. Seeing this, a log transformation is helpful because it makes the distribution more symmetrical.
Symmetrical distributions are balanced around the mean, so the model doesn't get influenced by outliers or skewed data. 